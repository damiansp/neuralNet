{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "democratic-collective",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from   torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from   torch.utils.data import DataLoader, TensorDataset\n",
    "from   torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "digital-socket",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0996,  0.0019,  0.0712, -0.0810],\n",
       "        [-0.0718, -0.0009, -0.0329, -0.0455],\n",
       "        [ 0.0598, -0.0455,  0.0094, -0.0337],\n",
       "        ...,\n",
       "        [ 0.1503, -0.0157,  0.0082, -0.0497],\n",
       "        [ 0.0816,  0.0203,  0.0449,  0.0351],\n",
       "        [ 0.0215,  0.0410,  0.0201, -0.0505]], requires_grad=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.randn(256, 4) / math.sqrt(256)\n",
    "W.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "prescription-pillow",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.zeros(4, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "removed-starter",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = F.cross_entropy\n",
    "# loss = loss_func(mod(X), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "characteristic-tourism",
   "metadata": {},
   "outputs": [],
   "source": [
    "#opt = optim.SGD(mod.parameter(), lr=ETA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "running-seating",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nopt.step()\\nopt.zero_grad()\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "with torch.no_grad():\n",
    "    for param in mod.parameters:\n",
    "        param -= param.grad * ETA # grad desc step\n",
    "    mod.zero_grad()\n",
    "'''\n",
    "\n",
    "# Equivalent to:\n",
    "'''\n",
    "opt.step()\n",
    "opt.zero_grad()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "israeli-heaven",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor X_batch, y_batch in train_dataloader:\\n    preds = mod(X_batch)\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batc_size=BATCH)\n",
    "'''\n",
    "\n",
    "# now instead of manually iterating through data like:\n",
    "'''\n",
    "for i in range((n - 1) // bs + 1):\n",
    "    X_batch = X_train[start:end]\n",
    "    y_batch = y_train[start:end]\n",
    "    preds = mod(X_batch)\n",
    "'''\n",
    "\n",
    "# ...can simplify to\n",
    "'''\n",
    "for X_batch, y_batch in train_dataloader:\n",
    "    preds = mod(X_batch)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "signed-shadow",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([1., 4., 2., 1., 3., 5.])\n",
    "points[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "median-seating",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(points[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "orange-stations",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "located-limit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1.0\n",
       " 4.0\n",
       " 2.0\n",
       " 1.0\n",
       " 3.0\n",
       " 5.0\n",
       "[torch.FloatStorage of size 6]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([[1., 4.], [2., 1.], [3., 5.]])\n",
    "points.storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "approved-nudist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 2]), torch.Size([3, 2]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points.shape, points.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "superior-floor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points.storage_offset() # index of 1st elem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "binary-garbage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points[1].storage_offset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "coated-capability",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points.stride() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "suburban-professor",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = torch.tensor([[1., 2.], [3., 4.]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "defensive-thread",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = torch.tensor(\n",
    "    [[1., 2.], [3., 4.]], dtype=torch.float32, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "irish-input",
   "metadata": {},
   "outputs": [],
   "source": [
    "#points2 = points.to(device='cuda')\n",
    "#points3 = points.to(device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "american-uzbekistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN for MNIST\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cn1 = nn.Conv2d(1, 16, 3, 1)\n",
    "        self.cn2 = nn.Conv2d(16, 32, 3, 1)\n",
    "        self.dp1 = nn.Dropout2d(0.1)\n",
    "        self.dp2 = nn.Dropout2d(0.25)\n",
    "        self.fc1 = nn.Linear(12 * 12 * 32, 64)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.cn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.cn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dp1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dp2(x)\n",
    "        x = self.fc2(x)\n",
    "        op = F.log_softmax(x, dim=1)\n",
    "        return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "covered-classic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(mod, device, train_dataloader, optim, epoch):\n",
    "    mod.train()\n",
    "    for i, (X, y) in enumerate(train_dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optim.zero_grad()\n",
    "        pred_prob = mod(X)\n",
    "        loss = F.nll_loss(pred_prob, y) # negative log likelihood\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        if i % 100 == 0:\n",
    "            print(f'epoch: {epoch} '\n",
    "                  f'[{i * len(X)}/{len(train_dataloader.dataset)} '\n",
    "                  f'({100. * i / len(train_dataloader):.2f}%)]\\t'\n",
    "                  f'training loss: {loss.item():.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "exciting-solid",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(mod, device, test_dataloader):\n",
    "    mod.eval()\n",
    "    loss = 0\n",
    "    success = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred_prob = mod(X)\n",
    "            loss += F.nll_loss(pred_prob, y, reduction='sum').item()\n",
    "            pred = pred_prob.argmax(dim=1, keepdim=True)\n",
    "            success += pred.eq(y.view_as(pred)).sum().item()\n",
    "    n = len(test_dataloader.dataset)\n",
    "    loss /= n\n",
    "    print(f'\\nTest dataset: Overall loss: {loss:.4f}, Overall Accuracy: '\n",
    "          f'{success}/{n} ({100*success / n:.2f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "sharing-complement",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = '../../../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "daily-technology",
   "metadata": {},
   "outputs": [],
   "source": [
    "NORM_MEAN = 0.1302 # train_X.mean() / 256.\n",
    "NORM_SD = 0.3069 # train_X.std() / 256.\n",
    "BATCH = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "refined-correction",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    datasets.MNIST(\n",
    "        DATA, \n",
    "        train=True, \n",
    "        download=True, \n",
    "        transform=transforms.Compose([\n",
    "            transforms.ToTensor(), \n",
    "            transforms.Normalize((NORM_MEAN,), (NORM_SD,))])\n",
    "    ),\n",
    "    batch_size = BATCH,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "suspected-religious",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(\n",
    "    datasets.MNIST(\n",
    "        DATA, \n",
    "        train=False, \n",
    "        transform=transforms.Compose([\n",
    "            transforms.ToTensor(), \n",
    "            transforms.Normalize((NORM_MEAN,), (NORM_SD,))])), \n",
    "    batch_size=500, \n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "boolean-rally",
   "metadata": {},
   "outputs": [],
   "source": [
    "ETA = 0.5\n",
    "\n",
    "torch.manual_seed(12)\n",
    "device = torch.device('cpu')\n",
    "mod = ConvNet()\n",
    "optimizer = optim.Adadelta(mod.parameters(), lr=ETA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "hybrid-flood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 [0/60000 (0.00%)]\ttraining loss: 0.006676\n",
      "epoch: 1 [3200/60000 (5.33%)]\ttraining loss: 0.007547\n",
      "epoch: 1 [6400/60000 (10.67%)]\ttraining loss: 0.000716\n",
      "epoch: 1 [9600/60000 (16.00%)]\ttraining loss: 0.077181\n",
      "epoch: 1 [12800/60000 (21.33%)]\ttraining loss: 0.081991\n",
      "epoch: 1 [16000/60000 (26.67%)]\ttraining loss: 0.006599\n",
      "epoch: 1 [19200/60000 (32.00%)]\ttraining loss: 0.018050\n",
      "epoch: 1 [22400/60000 (37.33%)]\ttraining loss: 0.009418\n",
      "epoch: 1 [25600/60000 (42.67%)]\ttraining loss: 0.013696\n",
      "epoch: 1 [28800/60000 (48.00%)]\ttraining loss: 0.017187\n",
      "epoch: 1 [32000/60000 (53.33%)]\ttraining loss: 0.310076\n",
      "epoch: 1 [35200/60000 (58.67%)]\ttraining loss: 0.053935\n",
      "epoch: 1 [38400/60000 (64.00%)]\ttraining loss: 0.018249\n",
      "epoch: 1 [41600/60000 (69.33%)]\ttraining loss: 0.001208\n",
      "epoch: 1 [44800/60000 (74.67%)]\ttraining loss: 0.014268\n",
      "epoch: 1 [48000/60000 (80.00%)]\ttraining loss: 0.011329\n",
      "epoch: 1 [51200/60000 (85.33%)]\ttraining loss: 0.017879\n",
      "epoch: 1 [54400/60000 (90.67%)]\ttraining loss: 0.114314\n",
      "epoch: 1 [57600/60000 (96.00%)]\ttraining loss: 0.185531\n",
      "\n",
      "Test dataset: Overall loss: 0.0346, Overall Accuracy: 9899/10000 (98.99%)\n",
      "epoch: 2 [0/60000 (0.00%)]\ttraining loss: 0.097620\n",
      "epoch: 2 [3200/60000 (5.33%)]\ttraining loss: 0.002595\n",
      "epoch: 2 [6400/60000 (10.67%)]\ttraining loss: 0.036290\n",
      "epoch: 2 [9600/60000 (16.00%)]\ttraining loss: 0.005886\n",
      "epoch: 2 [12800/60000 (21.33%)]\ttraining loss: 0.031665\n",
      "epoch: 2 [16000/60000 (26.67%)]\ttraining loss: 0.007877\n",
      "epoch: 2 [19200/60000 (32.00%)]\ttraining loss: 0.019259\n",
      "epoch: 2 [22400/60000 (37.33%)]\ttraining loss: 0.003288\n",
      "epoch: 2 [25600/60000 (42.67%)]\ttraining loss: 0.000594\n",
      "epoch: 2 [28800/60000 (48.00%)]\ttraining loss: 0.041952\n",
      "epoch: 2 [32000/60000 (53.33%)]\ttraining loss: 0.004679\n",
      "epoch: 2 [35200/60000 (58.67%)]\ttraining loss: 0.215777\n",
      "epoch: 2 [38400/60000 (64.00%)]\ttraining loss: 0.029993\n",
      "epoch: 2 [41600/60000 (69.33%)]\ttraining loss: 0.000252\n",
      "epoch: 2 [44800/60000 (74.67%)]\ttraining loss: 0.003778\n",
      "epoch: 2 [48000/60000 (80.00%)]\ttraining loss: 0.110844\n",
      "epoch: 2 [51200/60000 (85.33%)]\ttraining loss: 0.016041\n",
      "epoch: 2 [54400/60000 (90.67%)]\ttraining loss: 0.094055\n",
      "epoch: 2 [57600/60000 (96.00%)]\ttraining loss: 0.003763\n",
      "\n",
      "Test dataset: Overall loss: 0.0328, Overall Accuracy: 9900/10000 (99.00%)\n",
      "epoch: 3 [0/60000 (0.00%)]\ttraining loss: 0.037999\n",
      "epoch: 3 [3200/60000 (5.33%)]\ttraining loss: 0.006643\n",
      "epoch: 3 [6400/60000 (10.67%)]\ttraining loss: 0.005197\n",
      "epoch: 3 [9600/60000 (16.00%)]\ttraining loss: 0.065786\n",
      "epoch: 3 [12800/60000 (21.33%)]\ttraining loss: 0.005668\n",
      "epoch: 3 [16000/60000 (26.67%)]\ttraining loss: 0.005764\n",
      "epoch: 3 [19200/60000 (32.00%)]\ttraining loss: 0.016376\n",
      "epoch: 3 [22400/60000 (37.33%)]\ttraining loss: 0.000624\n",
      "epoch: 3 [25600/60000 (42.67%)]\ttraining loss: 0.036944\n",
      "epoch: 3 [28800/60000 (48.00%)]\ttraining loss: 0.057588\n",
      "epoch: 3 [32000/60000 (53.33%)]\ttraining loss: 0.012036\n",
      "epoch: 3 [35200/60000 (58.67%)]\ttraining loss: 0.141972\n",
      "epoch: 3 [38400/60000 (64.00%)]\ttraining loss: 0.003434\n",
      "epoch: 3 [41600/60000 (69.33%)]\ttraining loss: 0.010914\n",
      "epoch: 3 [44800/60000 (74.67%)]\ttraining loss: 0.046514\n",
      "epoch: 3 [48000/60000 (80.00%)]\ttraining loss: 0.032516\n",
      "epoch: 3 [51200/60000 (85.33%)]\ttraining loss: 0.004502\n",
      "epoch: 3 [54400/60000 (90.67%)]\ttraining loss: 0.011195\n",
      "epoch: 3 [57600/60000 (96.00%)]\ttraining loss: 0.125082\n",
      "\n",
      "Test dataset: Overall loss: 0.0350, Overall Accuracy: 9891/10000 (98.91%)\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train(mod, device, train_dataloader, optimizer, epoch + 1)\n",
    "    test(mod, device, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "particular-wealth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAM20lEQVR4nO3dXahc9bnH8d/vpCmI6UXiS9ik0bTBC8tBEo1BSCxbQktOvIjFIM1FyYHi7kWUFkuo2It4WaQv1JvALkrTkmMJpGoQscmJxVDU4o5Es2NIjCGaxLxYIjQRJMY+vdjLso0za8ZZa2ZN8nw/sJmZ9cya9bDMz7VmvczfESEAV77/aroBAINB2IEkCDuQBGEHkiDsQBJfGeTCbHPoH+iziHCr6ZW27LZX2j5o+7Dth6t8FoD+cq/n2W3PkHRI0nckHZf0mqS1EfFWyTxs2YE+68eWfamkwxFxJCIuSPqTpNUVPg9AH1UJ+zxJx6a9Pl5M+xzbY7YnbE9UWBaAivp+gC4ixiWNS+zGA02qsmU/IWn+tNdfL6YBGEJVwv6apJtsf8P2VyV9X9L2etoCULeed+Mj4qLtByT9RdIMSU9GxP7aOgNQq55PvfW0ML6zA33Xl4tqAFw+CDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJ9Dw+uyTZPirpnKRPJV2MiCV1NAWgfpXCXrgrIv5Rw+cA6CN244EkqoY9JO2wvcf2WKs32B6zPWF7ouKyAFTgiOh9ZnteRJywfb2knZIejIjdJe/vfWEAuhIRbjW90pY9Ik4Uj2ckPS1paZXPA9A/PYfd9tW2v/bZc0nflTRZV2MA6lXlaPxcSU/b/uxz/i8iXqilKwC1q/Sd/UsvjO/sQN/15Ts7gMsHYQeSIOxAEoQdSIKwA0nUcSNMCmvWrGlbu//++0vnff/990vrH3/8cWl9y5YtpfVTp061rR0+fLh0XuTBlh1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkuCuty4dOXKkbW3BggWDa6SFc+fOta3t379/gJ0Ml+PHj7etPfbYY6XzTkxcvr+ixl1vQHKEHUiCsANJEHYgCcIOJEHYgSQIO5AE97N3qeye9VtuuaV03gMHDpTWb7755tL6rbfeWlofHR1tW7vjjjtK5z127Fhpff78+aX1Ki5evFha/+CDD0rrIyMjPS/7vffeK61fzufZ22HLDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJcD/7FWD27Nlta4sWLSqdd8+ePaX122+/vZeWutLp9/IPHTpUWu90/cKcOXPa1tavX18676ZNm0rrw6zn+9ltP2n7jO3JadPm2N5p++3isf2/NgBDoZvd+N9LWnnJtIcl7YqImyTtKl4DGGIdwx4RuyWdvWTyakmbi+ebJd1Tb1sA6tbrtfFzI+Jk8fyUpLnt3mh7TNJYj8sBUJPKN8JERJQdeIuIcUnjEgfogCb1eurttO0RSSoez9TXEoB+6DXs2yWtK56vk/RsPe0A6JeO59ltPyVpVNK1kk5L2ijpGUlbJd0g6V1J90XEpQfxWn0Wu/Ho2r333lta37p1a2l9cnKybe2uu+4qnffs2Y7/nIdWu/PsHb+zR8TaNqUVlToCMFBcLgskQdiBJAg7kARhB5Ig7EAS3OKKxlx//fWl9X379lWaf82aNW1r27ZtK533csaQzUByhB1IgrADSRB2IAnCDiRB2IEkCDuQBEM2ozGdfs75uuuuK61/+OGHpfWDBw9+6Z6uZGzZgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJ7mdHXy1btqxt7cUXXyydd+bMmaX10dHR0vru3btL61cq7mcHkiPsQBKEHUiCsANJEHYgCcIOJEHYgSS4nx19tWrVqra1TufRd+3aVVp/5ZVXeuopq45bdttP2j5je3LatEdtn7C9t/hr/18UwFDoZjf+95JWtpj+m4hYVPw9X29bAOrWMewRsVvS2QH0AqCPqhyge8D2m8Vu/ux2b7I9ZnvC9kSFZQGoqNewb5K0UNIiSScl/ardGyNiPCKWRMSSHpcFoAY9hT0iTkfEpxHxL0m/k7S03rYA1K2nsNsemfbye5Im270XwHDoeJ7d9lOSRiVda/u4pI2SRm0vkhSSjkr6Uf9axDC76qqrSusrV7Y6kTPlwoULpfNu3LixtP7JJ5+U1vF5HcMeEWtbTH6iD70A6CMulwWSIOxAEoQdSIKwA0kQdiAJbnFFJRs2bCitL168uG3thRdeKJ335Zdf7qkntMaWHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSYMhmlLr77rtL688880xp/aOPPmpbK7v9VZJeffXV0jpaY8hmIDnCDiRB2IEkCDuQBGEHkiDsQBKEHUiC+9mTu+aaa0rrjz/+eGl9xowZpfXnn28/5ifn0QeLLTuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJMH97Fe4TufBO53rvu2220rr77zzTmm97J71TvOiNz3fz257vu2/2n7L9n7bPy6mz7G90/bbxePsupsGUJ9uduMvSvppRHxL0h2S1tv+lqSHJe2KiJsk7SpeAxhSHcMeEScj4vXi+TlJByTNk7Ra0ubibZsl3dOnHgHU4EtdG297gaTFkv4uaW5EnCxKpyTNbTPPmKSxCj0CqEHXR+Ntz5K0TdJPIuKf02sxdZSv5cG3iBiPiCURsaRSpwAq6SrstmdqKuhbIuLPxeTTtkeK+oikM/1pEUAdOu7G27akJyQdiIhfTyttl7RO0i+Kx2f70iEqWbhwYWm906m1Th566KHSOqfXhkc339mXSfqBpH229xbTHtFUyLfa/qGkdyXd15cOAdSiY9gj4m+SWp6kl7Si3nYA9AuXywJJEHYgCcIOJEHYgSQIO5AEPyV9Bbjxxhvb1nbs2FHpszds2FBaf+655yp9PgaHLTuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJMF59ivA2Fj7X/264YYbKn32Sy+9VFof5E+Roxq27EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBOfZLwPLly8vrT/44IMD6gSXM7bsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5BEN+Ozz5f0B0lzJYWk8Yj4re1HJd0v6YPirY9ExPP9ajSzO++8s7Q+a9asnj+70/jp58+f7/mzMVy6uajmoqSfRsTrtr8maY/tnUXtNxHxy/61B6Au3YzPflLSyeL5OdsHJM3rd2MA6vWlvrPbXiBpsaS/F5MesP2m7Sdtz24zz5jtCdsT1VoFUEXXYbc9S9I2ST+JiH9K2iRpoaRFmtry/6rVfBExHhFLImJJ9XYB9KqrsNueqamgb4mIP0tSRJyOiE8j4l+Sfidpaf/aBFBVx7DbtqQnJB2IiF9Pmz4y7W3fkzRZf3sA6tLN0fhlkn4gaZ/tvcW0RySttb1IU6fjjkr6UR/6Q0VvvPFGaX3FihWl9bNnz9bZDhrUzdH4v0lyixLn1IHLCFfQAUkQdiAJwg4kQdiBJAg7kARhB5LwIIfctc34vkCfRUSrU+Vs2YEsCDuQBGEHkiDsQBKEHUiCsANJEHYgiUEP2fwPSe9Oe31tMW0YDWtvw9qXRG+9qrO3G9sVBnpRzRcWbk8M62/TDWtvw9qXRG+9GlRv7MYDSRB2IImmwz7e8PLLDGtvw9qXRG+9GkhvjX5nBzA4TW/ZAQwIYQeSaCTstlfaPmj7sO2Hm+ihHdtHbe+zvbfp8emKMfTO2J6cNm2O7Z223y4eW46x11Bvj9o+Uay7vbZXNdTbfNt/tf2W7f22f1xMb3TdlfQ1kPU28O/stmdIOiTpO5KOS3pN0tqIeGugjbRh+6ikJRHR+AUYtr8t6bykP0TEfxfTHpN0NiJ+UfyPcnZE/GxIentU0vmmh/EuRisamT7MuKR7JP2vGlx3JX3dpwGstya27EslHY6IIxFxQdKfJK1uoI+hFxG7JV06JMtqSZuL55s19Y9l4Nr0NhQi4mREvF48Pyfps2HGG113JX0NRBNhnyfp2LTXxzVc472HpB2299gea7qZFuZGxMni+SlJc5tspoWOw3gP0iXDjA/Nuutl+POqOED3Rcsj4lZJ/yNpfbG7OpRi6jvYMJ077WoY70FpMcz4fzS57nod/ryqJsJ+QtL8aa+/XkwbChFxong8I+lpDd9Q1Kc/G0G3eDzTcD//MUzDeLcaZlxDsO6aHP68ibC/Jukm29+w/VVJ35e0vYE+vsD21cWBE9m+WtJ3NXxDUW+XtK54vk7Ssw328jnDMox3u2HG1fC6a3z484gY+J+kVZo6Iv+OpJ830UObvr4p6Y3ib3/TvUl6SlO7dZ9o6tjGDyVdI2mXpLcl/b+kOUPU2x8l7ZP0pqaCNdJQb8s1tYv+pqS9xd+qptddSV8DWW9cLgskwQE6IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUji3y9hG/l2EQpSAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_samples = enumerate(test_dataloader)\n",
    "b_i, (sample_data, sample_targets) = next(test_samples)\n",
    "plt.imshow(sample_data[0][0], cmap='gray', interpolation='none');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "solar-wales",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred: 7, Actual: 7\n",
      "Pred: 2, Actual: 2\n",
      "Pred: 1, Actual: 1\n",
      "Pred: 0, Actual: 0\n",
      "Pred: 4, Actual: 4\n",
      "Pred: 1, Actual: 1\n",
      "Pred: 4, Actual: 4\n",
      "Pred: 9, Actual: 9\n",
      "Pred: 5, Actual: 5\n",
      "Pred: 9, Actual: 9\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f'Pred: {mod(sample_data).data.max(1)[1][i]}', end=', ')\n",
    "    print(f'Actual: {sample_targets[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overall-spoke",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
